---
title: "R Notebook LadenÃƒÆ’Ã‚Â¶ffnungszeiten / EinsatzplÃƒÆ’Ã‚Â¤ne"
output:
  html_document:
    df_print: paged
  df_print: paged
  pdf_document: default
---
#Install packages if necessary
```{r}
# activate install if needed
#install.packages("RPostgreSQL")
#install.packages("tidyverse")
#install.packages("dplyr")
#install.packages("RColorBrewer")
```

#install libraries
```{r}
require("RPostgreSQL")
library(DBI)
library(tidyverse)
library(dplyr)
library(RColorBrewer)
```

#Create a connection to the Database
```{r}
# define the database connection string
DB_HOST='server2053.cs.technik.fhnw.ch'
DB_PORT = 5432
DB_DBNAME =  'warenkorb_db' # or'bank_db' 
DB_USERNAME = 'db_user'
DB_PASSWORD = 'db_user_pw'

con <- DBI::dbConnect(
  odbc::odbc(),
  Driver   = "PostgreSQL Unicode(x64)",
  Server   = DB_HOST,
  Database = DB_DBNAME,
  UID      = DB_USERNAME,
  PWD      = DB_PASSWORD,
  Port     = DB_PORT)


# load the PostgreSQL driver
drv <- dbDriver("PostgreSQL")
# connect to the database
con <- dbConnect(drv, dbname = DB_DBNAME,
                 host = DB_HOST, port = DB_PORT,
                 user = DB_USERNAME, password = DB_PASSWORD)
```


#Defines SQL Query
```{r}
orders <- dbGetQuery(con,  "SELECT * From orders")
orders_product_prior <- dbGetQuery(con,  "SELECT * From orders_product_prior")
orders_product_train <- dbGetQuery(con,  "SELECT * From orders_product_train")

```

```{r}
orders <- tbl(con,  "orders")
orders_product_prior <- tbl(con,  "orders_product_prior")
orders_product_train <- tbl(con,  "orders_product_train")
orders_product = tbl(con, "orders_product_prior") %>%
  union_all(tbl(con, "orders_product_train"))
```

#Plots (ggplot2)
##Orders over 24 hours

Als erstes wäre es interessant zu sehen, wie die Verteilung aller Bestellungen über den Zeitraum von 24 Stunden aussieht, da davon auszugehen ist, dass die Anzahl der Bestellungen pro uhrzeit variieren. 

```{sql, connection=con, output.var=orders_per_hour_per_day_sql}
SELECT order_dow, order_hour_of_day, COUNT(orders_id) 
FROM orders
GROUP BY order_dow, order_hour_of_day
```



```{r}
orders_per_hour_per_day <- orders %>%
  group_by(order_dow, order_hour_of_day) %>%
  summarise(nr_of_orders = as.numeric(n())) %>%
  collect()

  
ggplot(data = orders_per_hour_per_day) + 
  geom_col(mapping = aes(x = order_hour_of_day, y = nr_of_orders/7)) + 
  ggtitle("Total Nr of orders per hour") +
  xlab("Order hour") + ylab("Nr of orders")


```  
Aus dem Diagramm ist zu entnehmen, dass Bestellungen 24 Stunden lang pro Tag eingehen. Ab 6 Uhr ist ein Starker Anstieg der eingehenden Produkte bemerkbar............



##Orders over 7 days


```{r}

ggplot(data = orders_per_hour_per_day) + 
  geom_col(mapping = aes(x = order_dow, y = nr_of_orders, fill = as.factor(order_dow))) +
  ggtitle("Total Nr of orders per Day ") +
  xlab("Order day") + ylab("Nr of orders") +
  labs(fill='Weekday') +
  scale_fill_brewer(palette="Spectral")
  
```


```{r}
ggplot(data = orders) + 
  geom_boxplot(mapping = aes(x = order_dow, y = order_hour_of_day, group = order_dow, fill = as.factor(order_dow))) + 
  scale_fill_brewer(palette="Spectral") +
  labs(fill='Weekday') +
  ggtitle("Boxplot avg distribution of orders over the days over one week ")
```



##Orders over 24 hours and 7 days
```{r}
ggplot(data = orders_per_hour_per_day) + 
  geom_col(mapping = aes(x = order_hour_of_day, y = nr_of_orders, fill = as.factor(order_dow))) + 
  facet_wrap(~ order_dow,  nrow = 3) +
  labs(fill='Weekday') +
  scale_fill_brewer(palette="Spectral")
```


##Nr of items per order
Ziel: Untersuchung der Bestellgrössen.

```{sql, connection=con, output.var=items_per_order_sql}
SELECT X.orders_id, max(X.add_to_cart_order) FROM (
SELECT "index", orders_id, product_id, add_to_cart_order FROM orders_product_prior 
UNION 
SELECT "index", orders_id, product_id, add_to_cart_order FROM orders_product_train
) AS X 
GROUP BY X.orders_id
ORDER BY X.orders_id ASC 


```

```{r}
#Create a new table with Order_id and nr_of_items per order
items_per_order <- orders_product %>% 
  group_by(orders_id) %>%
  summarise(nr_of_items = max(add_to_cart_order))

ggplot(data = items_per_order) + 
  geom_bar(mapping = aes(x = nr_of_items), width=0.9) +
  xlab("Products per order") + ylab("Nr of orders") +
  coord_cartesian(xlim=c(0,75)) +
  ggtitle("Average Nr of products per order") 

mean_order_size <- mean(items_per_order$nr_of_items)
median_order_size <- median(items_per_order$nr_of_items)
max_order_size <- max(items_per_order$nr_of_items)
quantile_order_size <- quantile(items_per_order$nr_of_items)
```
Falls sehr viele grössere Bestellungen vorhanden sind, können diese den Arbeitsaufwand stark verändern. 

Das arithmetische Mittel über alle Bestellungen liegt bei 10.107 verschiedenen Produkten pro Bestellung.

Das Median über alle Bestellungen liegt bei `r median_order_size` verschiedenen Produkten pro Bestellung. Die hälfte aller Bestellungen enthällt entsprechend `r median_order_size` oder Weniger Produkte.

75% aller Bestellungen enthalten 14 oder weniger unterschiedliche Produkte.

Fazit: Um genauere Aussagen über Aussagen zum individuellen Auslastung machen zu können müssen erst die durchschnittlichen Bestellgrössen der einzelnen Stunden und Tage Analysiert werden.



```{r}
product_sales_per_week <- orders %>%
  group_by(orders_id) %>%
  left_join(select(items_per_order, orders_id, nr_of_items),by="orders_id")

```


##Items over 24 hours and 7 days
```{r}
ggplot(data = product_sales_per_week) + 
  geom_col(mapping = aes(x = order_hour_of_day, y = nr_of_items, fill = as.factor(order_dow))) +
  facet_wrap(~ order_dow,  nrow = 3) +
  labs(fill='Weekday') +
  xlab("Hour of Day") + ylab("Nr of Items") +
  scale_fill_brewer(palette="Spectral")
```


```{r}
products_per_hour_per_day <- orders %>%
  group_by(order_dow, order_hour_of_day) %>%
  left_join(select(orders_product, orders_id, product_id),by="orders_id") %>%
  summarise(nr_of_products = n()) %>%
  collect()

max_products <- products_per_hour_per_day %>%
  summarise(max_day = max(nr_of_products)) %>%
  summarise(max_week = max(max_day)) 
max_p <- (max_products$max_week)

max_orders <- orders_per_hour_per_day %>%
  summarise(max_day = max(nr_of_orders)) %>%
  summarise(max_week = max(max_day)) 
max_o <- (max_orders$max_week)
```

```{r}

order_product_ratio <- orders_per_hour_per_day %>%
  left_join(products_per_hour_per_day, by = c("order_dow" = "order_dow", "order_hour_of_day" = "order_hour_of_day")) %>%
  mutate(order_product_ratio  = nr_of_products/nr_of_orders, workload_orders = nr_of_orders/max_o, workload_products = nr_of_products/max_p)

```


Aussage: Die grösse der einzelnen Bestellungen beeinflusst direkt den Arbeitsaufwand pro Bestellung
Ziel: Durchschnittliche Bestellgrössen pro Stunde pro Tag. 


Aus dem folgenden Diagramm ist ersichtlich, dass die Bestellgrösse am Wochentagag 0 um 8:00 Uhr mit durchschnittlich 11.48 verschiedenen items am grössten ist.

Interessant ist, dass die Bestellgrössen über alle Wochentage von 20:00 bis 23:00 im Schnitt um 2 bis 2.5 items pro Bestellung ansteigen. Eine mögliche Erklärung wäre: Da bei instacart online Bestellungen mit home deliveries möglich sind, ein Teil abends nach der Arbeit so den einkauf erledigt.


```{r}
ggplot(data = order_product_ratio) + 
  geom_line(mapping = aes(x = order_hour_of_day, y = order_product_ratio, color=as.factor(order_dow), group = order_dow), size = 1) +
  ggtitle("Average ratio of products per order")  +
  labs(color='Weekday') +
  scale_color_brewer(palette="Spectral")
```



```{r}
ggplot(data = order_product_ratio) + 
  geom_line(mapping = aes(x = order_hour_of_day, y = workload_orders, color=as.factor(order_dow), group = order_dow), size = 1) +
  ggtitle("Average ratio orders")  +
  labs(color='Weekday') +
  scale_color_brewer(palette="Spectral")
```


```{r}
ggplot(data = order_product_ratio) + 
  geom_line(mapping = aes(x = order_hour_of_day, y = workload_products, color=as.factor(order_dow), group = order_dow), size = 1) +
  ggtitle("Average workload")  +
  labs(color='Weekday') +
  scale_color_brewer(palette="Spectral")
```

```{r echo=TRUE}
ggplot(data = order_product_ratio) + 
  geom_raster(mapping = aes(x = order_hour_of_day, y = order_dow, fill = workload_products)) +
  ggtitle("Average workload")+
  scale_fill_gradientn(colours=c("#000000","#0000FF","#00FF00","#FFFF00","#FF0000","#990000")) +
  labs(fill='Workload')
```

```{r}
Total_products <- sum(order_product_ratio$nr_of_products)
Total_orders <- sum(order_product_ratio$nr_of_orders)

```

```{r}
open_from_60 <- order_product_ratio %>%
  filter(workload_products >= 0.6)

P_contains_from_60 <- sum(open_from_60$nr_of_products)
P_loss_to_60 <- Total_products - P_contains_from_60

O_contains_from_60 <- sum(open_from_60$nr_of_orders)
O_loss_to_60 <- Total_orders - O_contains_from_60

P_Percent_loss_60 = P_loss_to_60/Total_products
P_Percent_contains_60 = P_contains_from_60/Total_products
```

```{r}
open_from_50 <- order_product_ratio %>%
  filter(workload_products >= 0.5)

P_contains_from_50 <- sum(open_from_50$nr_of_products)
P_loss_to_50 <- Total_products - P_contains_from_50

O_contains_from_50 <- sum(open_from_50$nr_of_orders)
O_loss_to_50 <- Total_orders - O_contains_from_50

P_Percent_loss_50 = P_loss_to_50/Total_products
P_Percent_contains_50 = P_contains_from_50/Total_products
```

```{r}
open_from_35 <- order_product_ratio %>%
  filter(workload_products >= 0.35) 


P_contains_from_35 <- sum(open_from_35$nr_of_products)
P_loss_to_35 <- Total_products - P_contains_from_35

O_contains_from_35 <- sum(open_from_35$nr_of_orders)
O_loss_to_35 <- Total_orders - O_contains_from_35

P_Percent_loss_35 = P_loss_to_35/Total_products
P_Percent_contains_35 = P_contains_from_35/Total_products

```

```{r}
open_from_25 <- order_product_ratio %>%
  filter(workload_products >= 0.25)

P_contains_from_25 <- sum(open_from_25$nr_of_products)
P_loss_to_25 <- Total_products - P_contains_from_25

O_contains_from_25 <- sum(open_from_25$nr_of_orders)
O_loss_to_25 <- Total_orders - O_contains_from_25

P_Percent_loss_25 = P_loss_to_25/Total_products
P_Percent_contains_25 = P_contains_from_25/Total_products
```

```{r}
open_from_15 <- order_product_ratio %>%
  filter(workload_products >= 0.15) %>%
  group_by(order_dow) %>%
  collect()

P_contains_from_15 <- sum(open_from_15$nr_of_products)
P_loss_to_15 <- Total_products - P_contains_from_15

O_contains_from_15 <- sum(open_from_15$nr_of_orders)
O_loss_to_15 <- Total_orders - O_contains_from_15

P_Percent_loss_15 = P_loss_to_15/Total_products
P_Percent_contains_15 = P_contains_from_15/Total_products

open_from_15 <- open_from_15 %>%
  group_by(order_dow) %>%
  summarise(c(open = min(order_hour_of_day)), (close = max(order_hour_of_day)))

open_time <- c(open_from_15$order_hour_of_day)
close_time <- c(open_from_15$order_hour_of_day)
```

```{r}
P_Percent_contains_60
P_Percent_loss_60

P_Percent_contains_50
P_Percent_loss_50

P_Percent_contains_35
P_Percent_loss_35

P_Percent_contains_25
P_Percent_loss_25

P_Percent_contains_15
P_Percent_loss_15

```



#Disconnect from the DB
```{r}
# close the connection (don't forget to cleanup)
dbDisconnect(con)
dbUnloadDriver(drv)
```

#Remove Variables from Global Environement
```{r}
remove(max)
```


